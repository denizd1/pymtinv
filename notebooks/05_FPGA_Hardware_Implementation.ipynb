{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 5: Hardware Implementation (FPGA) & Digital Logic\n",
                "\n",
                "**Topic:** PhD Thesis - Scientific Computing & Geophysical Inversion  \n",
                "**Description:** This final notebook bridges the gap between software algorithms and hardware reality. We translate the continuous Langevin Dynamics developed in previous notebooks into a **Digital Logic Architecture** suitable for FPGA implementation. This involves Fixed-Point Arithmetic, Pseudo-Random Number Generation (LFSR), and Massively Parallel Mesh design."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. From Continuous to Digital\n",
                "\n",
                "Standard CPUs use 64-bit Floating Point arithmetic (Float64). FPGAs, however, are optimized for low-precision integers. To map our P-bit algorithm to hardware, we must discretize both **Time** and **Values**.\n",
                "\n",
                "### 1.1. The Discrete Update Equation\n",
                "\n",
                "Recall the continuous Langevin equation from Notebook 3:\n",
                "\n",
                "$$\n",
                "m(t+\\Delta t) = m(t) - \\eta \\nabla \\Phi + \\sqrt{2T} \\cdot \\xi\n",
                "$$\n",
                "\n",
                "**Where:**\n",
                "* $m(t)$: Current model parameter (conductivity) at continuous time $t$.\n",
                "* $\\eta$: Learning rate (step size) determining how fast we move.\n",
                "* $\\nabla \\Phi$: Gradient of the energy function (Determinstic Force).\n",
                "* $T$: Temperature (Noise magnitude).\n",
                "* $\\xi$: Random Gaussian noise source.\n",
                "\n",
                "In **Digital Logic**, continuous variables become registers, and multiplications become bit-shifts. The hardware update rule is:\n",
                "\n",
                "$$\n",
                "M_{reg}[k+1] = M_{reg}[k] - (G_{fixed}[k] \\gg S) + (\\text{LFSR}[k] \\times \\text{Gain}[k])\n",
                "$$\n",
                "\n",
                "**Where:**\n",
                "* $M_{reg}[k]$: The model value stored in a hardware register at clock cycle $k$ (e.g., 16-bit Signed Integer).\n",
                "* $G_{fixed}[k]$: The gradient value converted to fixed-point integer format.\n",
                "* $\\gg S$: Bitwise Right Shift operation. This replaces the multiplication by learning rate $\\eta$. (e.g., shifting right by 4 is dividing by 16).\n",
                "* $\\text{LFSR}[k]$: A pseudo-random integer generated by a Linear Feedback Shift Register at cycle $k$.\n",
                "* $\\text{Gain}[k]$: An integer multiplier representing the Temperature $T$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Fixed-Point Arithmetic (Q-Format)\n",
                "\n",
                "We represent real numbers using integers by scaling them by a factor of $2^Q$. This is called **Q-format**.\n",
                "\n",
                "The conversion formula is:\n",
                "$$\n",
                "X_{int} = \\text{round}(X_{float} \\times 2^Q)\n",
                "$$\n",
                "\n",
                "**Where:**\n",
                "* $X_{int}$: The integer representation stored in FPGA registers (e.g., `1024`).\n",
                "* $X_{float}$: The real-world physical value (e.g., `1.0`).\n",
                "* $Q$: The number of fractional bits (Precision). If $Q=10$, then $2^{10}=1024$ becomes our scale factor.\n",
                "\n",
                "For example, if $Q=10$:\n",
                "* Real value $0.5$ $\\rightarrow$ Integer $512$.\n",
                "* Real value $-2.0$ $\\rightarrow$ Integer $-2048$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# --- Helper Functions for Hardware Simulation ---\n",
                "\n",
                "Q_FACTOR = 10  # 10 bits for fractional part\n",
                "SCALE = 2**Q_FACTOR\n",
                "\n",
                "def to_fixed(x):\n",
                "    \"\"\"Converts Float to Fixed-Point Integer (Q10)\"\"\"\n",
                "    return np.round(x * SCALE).astype(int)\n",
                "\n",
                "def to_float(x_int):\n",
                "    \"\"\"Converts Fixed-Point Integer back to Float\"\"\"\n",
                "    return x_int / SCALE\n",
                "\n",
                "# Test the quantization\n",
                "val_real = 0.12345\n",
                "val_fixed = to_fixed(val_real)\n",
                "val_recovered = to_float(val_fixed)\n",
                "\n",
                "print(f\"Real: {val_real}\")\n",
                "print(f\"Fixed (Integer): {val_fixed} (Binary: {bin(val_fixed)})\")\n",
                "print(f\"Recovered: {val_recovered}\")\n",
                "print(f\"Quantization Error: {abs(val_real - val_recovered):.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hardware Noise Generation: LFSR\n",
                "\n",
                "Generating true Gaussian noise on a chip is complex and resource-heavy. Instead, we use a **Linear Feedback Shift Register (LFSR)** to generate pseudo-random integers. While individual LFSRs are uniform, summing multiple LFSRs (or filtering one) roughly approximates a Gaussian distribution via the Central Limit Theorem.\n",
                "\n",
                "The update logic for a standard Galois LFSR is:\n",
                "\n",
                "$$\n",
                "b_{new} = b_{tap_1} \\oplus b_{tap_2} \\oplus \\dots \\oplus b_{tap_n}\n",
                "$$\n",
                "\n",
                "**Where:**\n",
                "* $\\oplus$: XOR operation (Exclusive OR gate in hardware).\n",
                "* $b_{new}$: The new bit shifted into the register (MSB or LSB).\n",
                "* $b_{tap}$: The bits at specific positions ('taps') chosen to maximize the period of the random sequence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LFSR16:\n",
                "    \"\"\"\n",
                "    16-bit Linear Feedback Shift Register Simulator.\n",
                "    Polynomial: x^16 + x^14 + x^13 + x^11 + 1\n",
                "    This logic is implemented using simple XOR gates and Flip-Flops on FPGA.\n",
                "    \"\"\"\n",
                "    def __init__(self, seed=1):\n",
                "        self.state = seed & 0xFFFF # Ensure 16-bit\n",
                "        \n",
                "    def next(self):\n",
                "        # Taps for 16-bit: 16, 14, 13, 11 (0-indexed: 15, 13, 12, 10)\n",
                "        bit = ((self.state >> 0) ^ (self.state >> 2) ^ (self.state >> 3) ^ (self.state >> 5)) & 1\n",
                "        self.state = (self.state >> 1) | (bit << 15)\n",
                "        # Convert unsigned to signed (centered around 0)\n",
                "        # Range: -32768 to +32767\n",
                "        return (self.state - 32768) \n",
                "\n",
                "# Verify Randomness\n",
                "lfsr = LFSR16(seed=1234)\n",
                "noise_samples = [lfsr.next() for _ in range(1000)]\n",
                "\n",
                "plt.figure(figsize=(10, 3))\n",
                "plt.plot(noise_samples[:200])\n",
                "plt.title(\"First 200 outputs of LFSR (Hardware Noise Source)\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The P-bit Core Logic (Bit-True Model)\n",
                "\n",
                "Now we assemble the components into a single **P-bit Core Class**. This class simulates exactly what the Verilog module does cycle-by-cycle.\n",
                "\n",
                "**Architecture Components:**\n",
                "1.  **Input:** `grad_input` (From gradient bus).\n",
                "2.  **Memory:** `m_reg` (Register storing the current conductivity model).\n",
                "3.  **Noise:** `LFSR` instance.\n",
                "4.  **Logic:** Adder, Shifter, and Clamper."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DigitalPbit:\n",
                "    def __init__(self, init_val, seed):\n",
                "        # Initialize register with fixed-point value\n",
                "        self.m_reg = to_fixed(init_val) \n",
                "        self.lfsr = LFSR16(seed)\n",
                "        \n",
                "    def clock_step(self, grad_input_fixed, shift_amount, noise_gain):\n",
                "        \"\"\"\n",
                "        Executes one clock cycle update.\n",
                "        grad_input_fixed: Gradient value (Integer)\n",
                "        shift_amount: Simulates Learning Rate (Bitwise Right Shift)\n",
                "        noise_gain: Multiplier for noise intensity (Temperature)\n",
                "        \"\"\"\n",
                "        # 1. Generate Noise (Hardware: LFSR module)\n",
                "        noise_val = self.lfsr.next() >> 4 # Scale down raw 16-bit noise\n",
                "        \n",
                "        # 2. Apply Update Rule (Hardware: Adder/Subtractor)\n",
                "        # Update = - (Grad >> S) + (Noise * Gain)\n",
                "        deterministic_force = -(grad_input_fixed >> shift_amount)\n",
                "        stochastic_force = noise_val * noise_gain\n",
                "        \n",
                "        self.m_reg += (deterministic_force + stochastic_force)\n",
                "        \n",
                "        # 3. Clamping (Hardware: Comparator/Mux)\n",
                "        # We must keep the model within physical limits [-4.0, 0.0]\n",
                "        # In Fixed-Point: [-4096, 0]\n",
                "        limit_min = to_fixed(-4.0)\n",
                "        limit_max = to_fixed(0.0)\n",
                "        \n",
                "        if self.m_reg < limit_min: self.m_reg = limit_min\n",
                "        if self.m_reg > limit_max: self.m_reg = limit_max\n",
                "        \n",
                "        return self.m_reg\n",
                "\n",
                "    def get_val(self):\n",
                "        return to_float(self.m_reg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Simulating a \"Bit-True\" Optimization\n",
                "\n",
                "We will now simulate a simple 1D optimization problem using this bit-true logic. This proves that our integer approximations still converge to the correct result.\n",
                "\n",
                "* **Goal:** Find $x$ that minimizes $f(x) = (x - (-2.0))^2$.\n",
                "* **Gradient:** $\\nabla f = 2(x + 2.0)$.\n",
                "* **Target Solution:** $x = -2.0$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulation Setup\n",
                "pbit = DigitalPbit(init_val=-4.0, seed=999)\n",
                "history = []\n",
                "noise_profile = []\n",
                "\n",
                "print(\"Starting Bit-True Hardware Simulation...\")\n",
                "\n",
                "CYCLES = 1500\n",
                "\n",
                "for cycle in range(CYCLES):\n",
                "    # 1. Read current value from register\n",
                "    x_curr = pbit.get_val()\n",
                "    \n",
                "    # 2. Compute Gradient (Simulating the external physics engine)\n",
                "    grad_float = 2 * (x_curr - (-2.0))\n",
                "    grad_fixed = to_fixed(grad_float)\n",
                "    \n",
                "    # 3. Annealing Schedule (Noise Gain drops over time)\n",
                "    # Linearly decrease noise gain from 3 to 0\n",
                "    current_noise_gain = int(3 * (1 - cycle/CYCLES))\n",
                "    \n",
                "    # 4. Clock Step (Hardware Update)\n",
                "    # Shift 6 -> Division by 64 (Learning Rate approx 0.015)\n",
                "    pbit.clock_step(grad_fixed, shift_amount=6, noise_gain=current_noise_gain)\n",
                "    \n",
                "    history.append(x_curr)\n",
                "    noise_profile.append(current_noise_gain)\n",
                "\n",
                "print(f\"Final Register Value: {history[-1]:.4f} (Target: -2.0000)\")\n",
                "\n",
                "# Visualization\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history, label='Register Value')\n",
                "plt.axhline(-2.0, color='r', linestyle='--', label='Target')\n",
                "plt.title(\"Optimization Convergence (Hardware Simulation)\")\n",
                "plt.xlabel(\"Clock Cycles\")\n",
                "plt.ylabel(\"Model Parameter\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(noise_profile, color='orange')\n",
                "plt.title(\"Annealing Schedule (Noise Gain)\")\n",
                "plt.xlabel(\"Clock Cycles\")\n",
                "plt.ylabel(\"Gain (Integer)\")\n",
                "plt.grid(True)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Massively Parallel Architecture (Mesh)\n",
                "\n",
                "The real power of this approach comes from scaling. On an FPGA, we instantiate thousands of `DigitalPbit` cores in a 2D Mesh topology.\n",
                "\n",
                "### 6.1. The Systolic Array Concept\n",
                "\n",
                "The FPGA design consists of:\n",
                "1.  **P-bit Core Mesh:** A $100 \\times 100$ grid of the registers defined above.\n",
                "2.  **Gradient Bus:** A high-bandwidth bus (AXI-Stream) that broadcasts gradient values from the CPU to the FPGA.\n",
                "3.  **Global Controller:** A simple state machine that manages the annealing schedule (broadcasting `shift_amount` and `noise_gain`).\n",
                "\n",
                "Since each P-bit updates independently based on its local gradient and internal random generator, the throughput scales **linearly** with the number of cores. Updating 10,000 parameters takes the same time as updating 1 parameter (1 clock cycle)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusion & Thesis Roadmap\n",
                "\n",
                "This notebook concludes the methodological development of the thesis. We have successfully traversed the full stack:\n",
                "\n",
                "1.  **Physics (Notebook 1):** Defined the Forward Problem (Maxwell's Eqs).\n",
                "2.  **Inverse Theory (Notebook 2):** Derived Gradients via Adjoint State Method.\n",
                "3.  **Statistical Mechanics (Notebook 3):** Introduced P-bits and Langevin Dynamics.\n",
                "4.  **Artificial Intelligence (Notebook 4):** Integrated RBMs for SciML inversion.\n",
                "5.  **Hardware Implementation (Notebook 5):** Mapped the algorithms to Digital Logic.\n",
                "\n",
                "The final proposed system represents a **Physics-AI-Hardware** co-design capable of solving large-scale geophysical problems with unprecedented speed and energy efficiency."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}